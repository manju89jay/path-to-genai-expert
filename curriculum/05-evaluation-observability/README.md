# 05 · Evaluation & Observability

**Overview:** Make quality measurable and repeatable.

**My project:** *(add link)*

**Objectives**
- Golden sets; faithfulness/hallucination checks.
- Retrieval metrics (Recall@k, nDCG), function-call accuracy.
- Traces, cost/latency dashboards.

**Pre-reqs**
- Modules 03–04 (recommended).

**Lessons**
- Offline evals before deploy; small online checks after.
- Failure taxonomy and regression suites.
- Add tracing early.

**Lab (starter idea)**
- Add ragas or promptfoo evals to your RAG/agent project; log traces and costs.

**Further resources**
- Ragas — https://docs.ragas.io/  • https://github.com/explodinggradients/ragas
- promptfoo — https://www.promptfoo.dev/  • https://github.com/promptfoo/promptfoo
- MLflow for GenAI — https://mlflow.org/docs/latest/llms/index.html
- Langfuse — https://docs.langfuse.com/
