# 06 · Serving & Inference

**Overview:** Run models reliably and efficiently.

**My project:** *(add link)*

**Objectives**
- Server vs gateway; batching/multiplexing; streaming.
- OpenAI-compatible routes and client ergonomics.
- Basic deploy patterns.

**Pre-reqs**
- Module 01.

**Lessons**
- When to self-host vs managed.
- Pinning versions/models.
- Simple rate limits and quotas.

**Lab (starter idea)**
- Deploy a small model on one server and hit it from your agent/RAG demo.

**Further resources (pick one stack)**
- NVIDIA Triton — https://docs.nvidia.com/deeplearning/triton/  • https://github.com/triton-inference-server/server
- TensorRT-LLM — https://nvidia.github.io/TensorRT-LLM/  • https://github.com/NVIDIA/TensorRT-LLM
- vLLM — https://vllm.ai/  • https://github.com/vllm-project/vllm
- Hugging Face TGI — https://huggingface.co/docs/text-generation-inference/index  • https://github.com/huggingface/text-generation-inference
